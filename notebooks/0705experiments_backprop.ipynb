{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "sys.path.append('../src/')\n",
    "\n",
    "from nodecore import Node\n",
    "from nodecore import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(object):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        self.W = Variable(\n",
    "            np.random.normal(0, 1 / out_ch, (out_ch, in_ch)).astype('f')\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.y = Node(linear_forward, [self.x, self.W])\n",
    "        return self.y\n",
    "\n",
    "    def backward(self):\n",
    "        self.W.grad = Node(linear_backward_W, [self.x, self.y.grad])\n",
    "        self.x.grad = Node(linear_backward_x, [self.W, self.y.grad])\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "def linear_forward(x, W):\n",
    "    y = np.matmul(x, W.T)\n",
    "    return y\n",
    "\n",
    "def linear_backward_W(x, gy):\n",
    "    gW = np.matmul(gy.T, x)\n",
    "    return gW\n",
    "\n",
    "def linear_backward_x(W, gy):\n",
    "    gx = np.matmul(gy, W)\n",
    "    return gx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Add(object):\n",
    "    def forward(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.y = Node(add_forward, [a, b])\n",
    "        return self.y\n",
    "\n",
    "    def backward(self):\n",
    "        self.a.grad = Node(add_backward, [self.y.grad])\n",
    "        self.b.grad = Node(add_backward, [self.y.grad])\n",
    "\n",
    "    def __call__(self, a, b):\n",
    "        return self.forward(a, b)\n",
    "\n",
    "def add_forward(a, b):\n",
    "    return a + b\n",
    "\n",
    "def add_backward(gy):\n",
    "    return gy\n",
    "\n",
    "def add(a, b):\n",
    "    node_add = Add()\n",
    "    return node_add(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sub(object):\n",
    "    def forward(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.y = Node(sub_forward, [a, b])\n",
    "        return self.y\n",
    "\n",
    "    def backward(self):\n",
    "        self.a.grad = Node(sub_backward_a, [self.y.grad])\n",
    "        self.b.grad = Node(sub_backward_b, [self.y.grad])\n",
    "\n",
    "    def __call__(self, a, b):\n",
    "        return self.forward(a, b)\n",
    "\n",
    "def sub_forward(a, b):\n",
    "    return a - b\n",
    "\n",
    "def sub_backward_a(gy):\n",
    "    return gy\n",
    "\n",
    "def sub_backward_b(gy):\n",
    "    return -gy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mul(object):\n",
    "    def forward(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.y = Node(mul_forward, [a, b])\n",
    "        return self.y\n",
    "\n",
    "    def backward(self):\n",
    "        self.a.grad = Node(mul_backward, [self.b, self.y.grad])\n",
    "        self.b.grad = Node(mul_backward, [self.a, self.y.grad])\n",
    "\n",
    "    def __call__(self, a, b):\n",
    "        return self.forward(a, b)\n",
    "\n",
    "def mul_forward(a, b):\n",
    "    return a * b\n",
    "\n",
    "def mul_backward(x, gy):\n",
    "    return x * gy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sum(object):\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.y = Node(sum_forward, [x])\n",
    "        return self.y\n",
    "\n",
    "    def backward(self):\n",
    "        self.x.grad = Node(sum_backward, [self.x, self.y.grad])\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "def sum_forward(x):\n",
    "    return np.sum(x)\n",
    "\n",
    "def sum_backward(x, gy):\n",
    "    return np.ones_like(x) * gy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "l0 = Linear( 5, 10)\n",
    "l1 = Linear(10, 10)\n",
    "l2 = Linear(10,  3)\n",
    "sub3 = Sub()\n",
    "mul4 = Mul()\n",
    "sum5 = Sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 真の係数\n",
    "W = np.random.random((3, 5)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 8\n",
    "x = Variable(np.random.random((batchsize, 5)).astype(np.float32))\n",
    "t = Variable(np.matmul(x.data, W.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 = l0(x)\n",
    "h1 = l1(h0)\n",
    "y  = l2(h1)\n",
    "\n",
    "dif_y_t = sub3(y, t)\n",
    "loss = sum5(mul4(dif_y_t, dif_y_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.grad = Variable(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum5.backward()\n",
    "mul4.backward()\n",
    "sub3.backward()\n",
    "l2.backward()\n",
    "l1.backward()\n",
    "l0.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.17448576, -0.12396366, -0.17368382, -0.21801567, -0.13547616],\n",
       "       [ 0.23901932,  0.1784434 ,  0.28673586,  0.3453216 ,  0.27966982],\n",
       "       [ 0.18470582,  0.12594897,  0.16575038,  0.19146392,  0.22489803],\n",
       "       [ 0.4409402 ,  0.33127028,  0.54326725,  0.6459553 ,  0.568679  ],\n",
       "       [ 0.21001351,  0.15537557,  0.24353132,  0.2961584 ,  0.22539008],\n",
       "       [ 0.98166025,  0.70743775,  1.0592948 ,  1.2572393 ,  1.173618  ],\n",
       "       [-0.26472995, -0.19528773, -0.30999815, -0.36450174, -0.35181075],\n",
       "       [-1.657927  , -1.1968415 , -1.7898829 , -2.1443279 , -1.8770658 ],\n",
       "       [-0.4020802 , -0.29051042, -0.43778646, -0.51931304, -0.4849494 ],\n",
       "       [-0.94823027, -0.683903  , -1.0218748 , -1.2215724 , -1.0864532 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l0.W.grad.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習テスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43622115\n",
      "0.34013852\n",
      "0.27952713\n",
      "0.46903676\n",
      "0.38006333\n",
      "0.43638194\n",
      "0.2947218\n",
      "0.50222605\n",
      "0.38708603\n",
      "0.28479886\n",
      "0.5035801\n",
      "0.4812018\n",
      "0.48898917\n",
      "0.3695941\n",
      "0.3453398\n",
      "0.39792928\n",
      "0.37899005\n",
      "0.46050546\n",
      "0.46633\n",
      "0.3511302\n",
      "0.63206553\n",
      "0.49406764\n",
      "0.4064721\n",
      "0.28691635\n",
      "0.4381137\n",
      "0.5857443\n",
      "0.38138482\n",
      "0.48468617\n",
      "0.45428178\n",
      "0.28868562\n",
      "0.31658155\n",
      "0.2790502\n",
      "0.32902244\n",
      "0.35131475\n",
      "0.5700266\n",
      "0.42452258\n",
      "0.42105484\n",
      "0.38461855\n",
      "0.22674334\n",
      "0.25425643\n",
      "0.35525084\n",
      "0.38023925\n",
      "0.35353464\n",
      "0.26514643\n",
      "0.41488856\n",
      "0.48902184\n",
      "0.3090096\n",
      "0.3125139\n",
      "0.36662334\n",
      "0.36845058\n",
      "0.40047967\n",
      "0.29440707\n",
      "0.29390767\n",
      "0.39879692\n",
      "0.36840886\n",
      "0.26612264\n",
      "0.3250206\n",
      "0.40592158\n",
      "0.4503349\n",
      "0.31340742\n",
      "0.35602528\n",
      "0.38381612\n",
      "0.25926873\n",
      "0.17217898\n",
      "0.4756266\n",
      "0.2098824\n",
      "0.36401635\n",
      "0.45687523\n",
      "0.34307975\n",
      "0.43184093\n",
      "0.46871236\n",
      "0.39925766\n",
      "0.23448831\n",
      "0.29725498\n",
      "0.2907148\n",
      "0.24454471\n",
      "0.30848056\n",
      "0.45815206\n",
      "0.2201158\n",
      "0.24983609\n",
      "0.20530328\n",
      "0.5917789\n",
      "0.25915045\n",
      "0.456494\n",
      "0.33413425\n",
      "0.3958828\n",
      "0.26985157\n",
      "0.23671898\n",
      "0.22187178\n",
      "0.336352\n",
      "0.3454002\n",
      "0.28728074\n",
      "0.26518518\n",
      "0.44972157\n",
      "0.3650537\n",
      "0.19181515\n",
      "0.3315544\n",
      "0.32772434\n",
      "0.35757363\n",
      "0.42445335\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "\n",
    "for i in range(100):\n",
    "    # make batch\n",
    "    x = Variable(np.random.random((batchsize, 5)).astype(np.float32))\n",
    "    t = Variable(np.matmul(x.data, W.T))\n",
    "    \n",
    "    # forward\n",
    "    h0 = l0(x)\n",
    "    h1 = l1(h0)\n",
    "    y  = l2(h1)\n",
    "\n",
    "    # loss\n",
    "    dif_y_t = sub3(y, t)\n",
    "    loss = sum5(mul4(dif_y_t, dif_y_t))\n",
    "    \n",
    "    # backward\n",
    "    loss.grad = Variable(1)\n",
    "    \n",
    "    sum5.backward()\n",
    "    mul4.backward()\n",
    "    sub3.backward()\n",
    "    l2.backward()\n",
    "    l1.backward()\n",
    "    l0.backward()\n",
    "    \n",
    "    # update\n",
    "    l0.W.data -= lr * l0.W.grad.data\n",
    "    l1.W.data -= lr * l1.W.grad.data\n",
    "    l2.W.data -= lr * l2.W.grad.data\n",
    "    \n",
    "    print(loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.5498765 , 1.1681993 , 1.0430547 ],\n",
       "        [1.6770533 , 1.2405598 , 1.0832026 ],\n",
       "        [1.7349929 , 1.2710277 , 1.1185577 ],\n",
       "        [2.0274627 , 1.5346563 , 1.3645009 ],\n",
       "        [1.4642233 , 1.099669  , 0.9885646 ],\n",
       "        [1.3707267 , 1.0048237 , 0.86772615],\n",
       "        [2.4010842 , 1.8111888 , 1.6130209 ],\n",
       "        [1.9897652 , 1.4677737 , 1.2769117 ]], dtype=float32),\n",
       " array([[1.7393267, 1.1519265, 0.8866402],\n",
       "        [1.797891 , 1.4282349, 1.2312665],\n",
       "        [1.6633149, 1.0550609, 1.2571576],\n",
       "        [2.1259358, 1.7245108, 1.2852108],\n",
       "        [1.5034199, 1.226975 , 1.1889428],\n",
       "        [1.4598958, 1.1640102, 1.0277932],\n",
       "        [2.3565445, 1.9576244, 1.6499039],\n",
       "        [2.0206528, 1.3398588, 1.1761358]], dtype=float32))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.data, t.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W.grad = Node(backward, [hoge])と代入しているがこれは加算にしないといけない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
